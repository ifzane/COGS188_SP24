{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"We\" learn Catan\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Tom Hocquet\n",
    "- Ian Zane\n",
    "- Kai Stern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "In this project, we aim to develop a reinforcement learning (RL) model that can learn to play Settlers of Catan, a multiplayer board game about trading and resource management. Specifically, we aim to use this model to note how the evolution of group dynamics and play styles in a set of agents that repeatedly play each other might be different than agents trained for a more game theory optimal strategy. We will be using a virtual environment to train a singular cohort of agents. The same set of agents will repeatedly play against each other, creating an environment similar to how many people would play board games. Throughout the training process, example games will be collected to illustrate the process and after a significant number of training iterations, the model will be evaluated through performance comparisons between agents with different exploration rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Settlers of Catan (aka Catan), a popular strategy board game, presents a rich and dynamic environment for RL implementation. Catan is a multiplayer game where each individual is aiming to utilize resources to build out the largest empire possible. With its combination of resource management, negotiation, and strategic placement, it offers a complex yet structured domain. Successful RL agents for Catan learn probabilistic decision making to select high value locations to build in, long term strategy to plan out achieving long roads or large armies, and successful negotiation to make deals with opposing players. RL or DL methods have been used before to teach an algorithm to play Catan, often using different implementations. The problem has been solved using Q-learning methods [1](#kimnote) with MDPs [2](#pfeiffernote), and also using a Monte-Carlo search [3](#chaslotnote). Even though this problem has been solved before, the diversity of methods to solve this problem and the variety of different possible strategies make this problem still relevant to solve. There are many unique solutions to be found as there is no “correct” approach to winning the game, especially with its 244,432,188,000 [4](#dusautoynote) possible different starting boards, which will hopefully promote a lot of different strategies being developed. We should also be able to, using smart code to minimize complexity, be able to solve our problem with our available resources. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "We are using Q-learning to train multiple agents on the board game Settlers of Catan. Success in the game is heavily reliant on interactions between the players and the environment. Players can trade with the bank, steal from each other, block others from building or getting resources, and much more. An effective agent must learn strategies to navigate the complex, stochastic environment of multiplayer Catan, balancing short-term, long-term, and minimax strategies. Statistics on agent success rates, such as win rate and average points per game, provide quantifiable performance metrics. Catan can be played repeatedly from new or previous starting points to provide necessary reproducibility for research.\n",
    "\n",
    "## Environment\n",
    "\n",
    "Settlers of Catan is played on the fictional island of Catan. The board is made up of 20 hexagonal tiles, each representing different types of terrain such as forests, mountains, fields, hills, pastures, and deserts. Each tile is numbered and produces a specific resource (wood, brick, wheat, sheep, or ore) when its number is rolled. The island is surrounded by water, which the players cannot enter. Some tiles bordering the ocean contain trading ports where players can trade specific resources.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of the game is to reach 10 victory points. Points are accumulated through various achievements, including building settlements and cities, buying development cards, creating the longest road on the island, or having the largest army on the island. The game continues until a player reaches 10 victory points.\n",
    "\n",
    "## Turns\n",
    "\n",
    "A player begins their turn by either rolling two six-sided dice or playing a development card (the only action that can be done before rolling). The dice roll determines which tiles produce resources that turn. After that, the player can bargain with their opponents, build cities, roads, or settlements, or buy development cards. If the player rolls a 7, each player with 8 or more cards loses half of them. The player who rolled the 7 also moves the robber character to any tile on the map, blocking it from producing resources until the robber is moved again. When the robber is moved, the player who moved it can steal a resource card at random from a player with infrastructure on the tile where the robber ends up. Cities and settlements increase the amount of resources a player receives from the tile they are located on and add to the player's victory points. Roads connect a player’s infrastructure, and new infrastructure can only be built if it is connected to a road. Development cards provide the player with victory points or special abilities. If a player controls (has built infrastructure up to) specific trading ports on the edge of the map, they can trade resources with the resource bank during their turn if they choose to.\n",
    "\n",
    "## Development Cards\n",
    "\n",
    "- **Victory Point**: Gain one victory point.\n",
    "- **Knight**: Move the robber.\n",
    "- **Year of Plenty**: Collect 2 resource cards of your choice from the bank.\n",
    "- **Monopoly**: Select one resource type and gain all cards of that type from your opponents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Since we are doing a reinforcement learning task on a game, rather than a dataset we have an environment that models the board game Settlers of Catan. The format of the game was presented in the problem statement, and we settled on the implementation by Karan Vombatkere. This environment provides a GUI to visualize the game board and player moves, python files for the set up of the game, terminal log outputs for simple summaries, and several heuristic based ai examples. This environment did not provide code using the algorithm we implemented: Q Learning. Because of the heuristic nature of the provided algorithms we had to add methods allowing for Q Learning updating and repeated training. Additionally, we built our own QLearningPlayer and QLearningGame python files to enable AI rather than heuristic decision making. The link to the original environment is provided below.\n",
    " \n",
    "https://github.com/kvombatkere/Catan-AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We used Q Learning to train our RL agents how to play Settlers of Catan. Catan’s complexity as a game makes model free algorithms like Q Learning particularly useful as they eliminate the need for a complicated model to guide learning. Agents learn via experience playing the game many times over. Dice rolls ensure that there is a significant amount of randomness in Catan. With two dice numbers like 6, 7, and 8 are rolled more frequently than numbers like 2, 3, or 12. This increases the value of resource tiles on highly probable numbers. Q Learning can learn value functions based on observed rewards to adjust the expected probability of outcomes and learn winning strategies for stochastic environments. Long-term strategy is critical for success in Catan. Players have to make moves that don’t provide immediate rewards to set up high-reward future plays. Q Learning updates incrementally enabling it to capture longer term strategy. Additionally, we implemented the Epsilon-Greedy strategy where the agent takes a random action with probability Epsilon and the expected optimal action with probability Epsilon - 1. This enables the agent to learn effective strategy while exploring moves that could lead to new understanding of what is optimal in a given state action pair. We trained the Q Learning agents by having them play against each other in successive games. Our Q Learning agents had three differing Epsilon (exploration rate) values: 0.1, 0.2, and 0.3. We compare the three varyingly exploratory agents in the results and discussion section of this report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We used multiple evaluation metrics for our project. The first is the most straightforward: Win Rate. Quantized as games won divided by total games, win rate provides a metric to broadly evaluate the strategy of each of the agents we end up training at Catan. The goal for each agent is to win as many games as possible and this metric enables measuring alignment with that goal. The second evaluation metric is average victory points obtained per game. This metric evaluates how well each agent did per game regardless of whether it won. It will provide differentiation for strategies that win a similar percentage of the time and clarify how consistently a strategy places an agent in the top spots.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "### Subsection 1: Varying Discount Factor\n",
    "\n",
    "We differentiated our 3 agents by changing hyperparameter values to give them different perspectives on solving Catan. The first hyperparameter we varied was the discount factor, which affects whether the agent prioritizes short or long-term rewards. The dictionary below indicates the total number of games won per agent (e.g. Tom won 390 games out of the 1250 games). The two below are in order: victory points per game and reward per game. As we can see, the reward construction we developed is a relatively good estimate of the performance. The agent with the highest reward won the most games and had the highest average score over a long series of games. The same was true of the agent with the lowest in all categories.\n",
    "\n",
    "Each of these agents was trained over 1,250 episodes with different discount factor hyperparameter values. The agent Tom had a discount factor of 0.3, Kai had 0.6, and Ian had 0.9. The agent Tom prefers short-term rewards, Kai is fairly balanced between short and long term, and Ian prefers long-term rewards. We can see that the agent focusing on long-term rewards performed the best of the three overall.\n",
    "\n",
    "### Subsection 2: Varying Exploration Rate\n",
    "\n",
    "The second hyperparameter that we varied between the agents was the exploration rate. This represents the value of Epsilon in the Epsilon-Greedy algorithm and determines the degree to which each agent takes exploratory or expected optimal actions. The Tom agent was trained with an exploration rate of 0.1, Kai was trained with an exploration rate of 0.2, and Ian was trained with an exploration rate of 0.3. This means that Ian takes random (exploratory) actions the most frequently while Tom takes them the least.\n",
    "\n",
    "Interestingly, the agent with a small or big exploratory rate seems to be performing better than the middle of the line rate. The agent with low exploratory rate starts with a relatively high reward, but while the reward remains high, it doesn’t learn mechanics that help it win games, as it ends up losing out to the most exploratory agent (Ian) in the long run. We can see this agent starts off with low rewards, which then increase dramatically and converge to around 12 reward points per game. This is the most interesting of the three agents. It seemed to have learned something really good that might not have been the best strategy, as it plummets quickly and is slowly rising in value towards the end.\n",
    "\n",
    "For the last agent with the highest exploratory rate, we can see a graph that has less stability than the other two. It changes directions more often, likely due to it trying new random things to see if they work. While this agent did not have the highest average reward (which makes sense with a high exploratory policy), it did win the most games and had the second-highest average points per game. This shows that exploratory agents can perform well on complex tasks with a significant number of potential actions, such as Catan.\n",
    "\n",
    "### Subsection 3: Varying Learning Rate\n",
    "\n",
    "The third hyperparameter we tested is the learning rate. This is a multiplier to learning which can help learn faster in some scenarios with the possible downside of missing the convergence. Here we tried learning rates of 0.1, 0.2, and 0.3 for AI agents Tom, Kai, and Ian respectively.\n",
    "\n",
    "We can see that the agent with the highest learning rate, Ian, had the most wins, the most points per game, and the highest average reward, showing what seems to be a better choice for the learning rate hyperparameter.\n",
    "\n",
    "Here we can see a graph similar to those in Subsection 2, which makes sense since this has all the “default” parameters. It reached a mean of 6.7 victory points per game with an average reward of 8.5.\n",
    "\n",
    "We can see that the agent Kai reaches a higher reward faster but with a lower average reward.\n",
    "\n",
    "In this case, AI Ian almost immediately reaches an incredibly high reward at an average of 19 per game compared to the others, which peaked somewhere between 10 and 12. However, it is unable to maintain this high average reward, and while the reward decays quickly and stabilizes, it still stabilizes higher than either Tom or Kai at an average reward of 13.2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "The number of turns it takes to win dramatically decreases with the number of games run, with the first games being mostly around 100 (sometimes upwards of 600!) turns and as more games are run they creep down to around 40 turns on average with some games being as low as 20 turns. This shows that the agents learn more effective strategies over time and therefore require fewer turns to win. \n",
    "\n",
    "The results of Subsection One demonstrated the effect of the discount factor on agent performance. The agent with the lowest discount factor won the most games; however, the agent with the highest discount factor had higher average points and average rewards per game. We suspect that this is a result of the agent that preferred short-term rewards winning shorter games, but that if games run longer, the agent that prefers long-term rewards outperforms the other two. Enough games ended quickly for the low discount factor agent to win the most overall. The lowest and highest discount factor agents outperformed the medium discount factor agent. This could be a result of there being advantages to going for easy, short-term rewards and high-payout long-term rewards but not for combining both strategies. It appears to be optimal for an agent to choose one or the other and stick to it.\n",
    "\n",
    "Subsection Two differentiated the agents based on exploration rate. Surprisingly, the most exploratory agent, which takes the most random actions as opposed to predicted optimal ones, won the most games. This shows that in complex environments such as Catan, there are advantages to exploring many different possible actions and finding new ways to play the game.\n",
    "\n",
    "Subsection Three talked about differing learning rates, and we saw that a higher learning rate tended to lead to a higher average reward and points per game. This makes sense as our reward function only gives out large rewards for scoring points, which are relatively rare events in Catan. This leads to the agent with the higher learning rate being able to better assimilate the information it learned from the reward. Overall, each of the agents performed fairly similarly given the different exploration rate values. The only metric that differed significantly between the agents was points per game, where the least exploratory agent outperformed the other two. This made sense given that this agent takes the expected optimal action most frequently.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "We had limitations due to the complexity of certain features and not being able to resolve them before the deadline, so not all implementations of Catan are done by Q-Learning. The knight follows a heuristic function, the roadbuilder card is removed from the game, and the players do not trade among themselves. While these features are important to the game, we would not have been able to do any kind of learning with features that would break the code. All aspects above are ways our project can be improved.\n",
    "\n",
    "Our final limitation was from running into infinite games, which essentially happened when the agents blocked each other or never had enough to upgrade their settlements to cities, which made us lose some graphs over all the iterations. In theory, there are solutions for this in the game, as agents could learn to trade for resources they couldn’t get with the bank, but the chance to reach that point would require a series of exploration decisions back to back which didn’t occur in the number of trials we were able to run. An alternative way to catch this could be to add catches if the game is more than 1000 turns to stop the agent from reinforcing bad decisions in this way.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Our project has a very low chance of violating ethics or privacy standards. It incorporates no user data that would be at risk of being leaked. The multiple agents being trained are constrained within their task environment and cannot have any impact beyond it. The software we used for the project is open source and provided with the intention of free use. The environment specifies the inclusion of a license in any projects using it, and we included said license.\n",
    "\n",
    "One potential area of impact for the project is on the online Settlers of Catan community. There are many people who play the game recreationally and even some who play professionally. We do not aim to disrupt these communities with our work. As seen in other games, input from AI can have positive and negative effects. In chess, computers have enabled deeper learning of the game while also enabling cheaters to win unfairly. We have no intention of releasing our Catan project publicly but are prepared to work towards developing ways to catch cheaters using AI if our project ends up helping cheaters make decisions in their games. This approach is similar to how cheating is regulated in online chess. There already exist bots that can play Settlers of Catan, so the community of online players playing the game have had some time to adapt to the idea of playing against computers as well as humans.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Through this project, we were able to differentiate hyperparameters between agents to determine which values enabled agents to succeed. Changes in the discount factor led to significant differences between the agents. Agent Ian (with the highest discount factor) had a roughly 15% higher chance on average of winning than the next highest agent. This makes sense for a game like Catan, where there’s significant value in setting up high-value future turns through acquiring resources needed to take actions rather than just taking the action that has the most value in the short term.\n",
    "\n",
    "The second hyperparameter we experimented with was the exploration rate. This didn’t have as strong an impact as eventually all agents approximately converged. Notably, a middling exploration rate provided an effective balance between taking short and long-term actions in managing the reward variance but was slower to reach higher reward values.\n",
    "\n",
    "The third hyperparameter we varied was the learning rate. In this case, the agents with higher learning rates learned faster and were more successful overall. Future research on the efficacy of Q Learning for training AI agents on Settlers of Catan could add AI decision making into more of the game decisions or could compare standard Q Learning with other algorithms like Deep Q Learning or Double Q Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes\n",
    "<a name=\"kimnote\"></a>1.[^](#kim): Kim, Chris, and Aaron Li. *Re-l Catan: Evaluation of Deep Reinforcement Learning For ...*. cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf. Accessed 6 May 2024.<br> \n",
    "<a name=\"pfeiffernote\"></a>2.[^](#pfeiffer): Pfeiffer, Michael. “Reinforcement Learning of Strategies for Settlers of Catan.” Graz University of Technology, Univ. of Wolverhampton, 1 Jan. 1970, graz.elsevierpure.com/en/publications/reinforcement-learning-of-strategies-for-settlers-of-catan-2.<br>\n",
    "<a name=\"chaslotnote\"></a>3.[^](#chaslot): Chaslot, Guillaume. “(PDF) Monte-Carlo Tree Search in Settlers of Catan.” Monte-Carlo Tree Search in Settlers of Catan, 2009, www.researchgate.net/publication/220716999_Monte-Carlo_Tree_Search_in_Settlers_of_Catan.<br>\n",
    "<a name=\"dusautoynote\"></a>4.[^](#dusautoy): Du Sautoy, Marcus. 244,432,188,000 Games of Catan - Numberphile, 2023, www.youtube.com/watch?v=NkNoVnXfVXI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
