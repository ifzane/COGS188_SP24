{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Tom Hocquet\n",
    "- Ian Zane\n",
    "- Kai Stern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "In this project, we aim to develop a reinforcement learning (RL) model that can learn to play Settlers of Catan, a multiplayer board game about trading and resource management. Specifically, we aim to use this model to note how the evolution of group dynamics and play styles in a set of agents that repeatedly play each other might be different than agents trained for a more game theory optimal strategy. We will be using a virtual environment to train a singular cohort of agents. The same set of agents will repeatedly play against each other, creating an environment similar to how many people would play board games. Throughout the training process, example games will be collected to illustrate the process and after a significant number of training iterations, the model will be evaluated through comparisons to an online Settlers of Catan set of bots, a game with humans, and comparison to additional models developed by other independent programmers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Settlers of Catan, a popular strategy board game, presents a rich and dynamic environment for RL implementation. With its combination of resource management, negotiation, and strategic placement, it offers a complex yet structured domain. RL or DL methods have been used before to teach an algorithm to play Catan, often using different implementations. The problem has been solved using Q-learning methods<a href=\"#firstnote\"><sup>[1]</sup></a> with MDPs<a href=\"#secondnote\"><sup>[2]</sup></a>, and also using a Monte-Carlo search<a href=\"#thirdnote\"><sup>[3]</sup></a>. Even though this problem has been solved before, the diversity of methods to solve this problem and the variety of different possible strategies make this problem still relevant to solve. There are many unique solutions to be found as there is no “correct” approach to winning the game, especially with its 244,432,188,000<a href=\"#fourthnote\"><sup>[4]</sup></a> possible different starting boards, which will hopefully promote a lot of different strategies being developed. We should also be able to, using smart code to minimize complexity, be able to solve our problem with our available resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are using reinforcement learning to train multiple agents on the board game Settlers of Catan (aka Catan). Catan is a multiplayer strategy game where each individual is aiming to utilize resources to build out the largest empire possible. Success in the game is heavily reliant on interaction between the players through bargaining. An effective agent must learn to negotiate with the other players to get the resources they need to succeed. This project expands upon classic reinforcement learning tasks with one agent learning to succeed in an environment as the social negotiation aspect of Catan forces each agent to learn how to successfully play off of each other in addition to learning to play successfully within the rules of the game.\n",
    "\n",
    "Environment:  Settlers of Catan is played on the fictional island Catan. The board is made up of 20 hexagonal tiles each representing types of terrain such as forests, mountains, fields, hills, pastures, and deserts. Each tile is numbered and produces a specific resource (wood, brick, wheat, sheep, or ore) when its number is rolled The island is surrounded by water which the players cannot enter. \n",
    "\n",
    "Objective: The goal of the game is to reach 10 victory points. Points are accumulated through achieving various goals. These include using resources to build settlements and cities, buying development cards, creating the longest road on the island, or having the largest army on the island. \n",
    "\n",
    "Turns: A player begins their turn by rolling two six sided dice or playing a development card (the only action that can be done before rolling). Once the dice are rolled, it determines which tiles produce resources that turn. After that the player can bargain with their opponents, build cities, roads, or settlements, or buy development cards. If the player rolls a 7, each player with 8 or more cards removes half. They also move the robber character to any tile on the map. That tile cannot produce resources until the robber is moved. Cities and settlements increase the amount of resources a player receives from the tile they are located in and add to the player's victory points. Roads connect a player’s infrastructure. Players cannot build new infrastructure unless it is connected to a road. Development cards can provide the player with victory points or special abilities. If a player controls (has built infrastructure up to) specific trading ports on the edge of the map then they can trade resources with the resource bank during their turn if they so choose. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Since we are doing a reinforcement learning task on a game, rather than a dataset we have an environment that models the board game Settlers of Catan. The format of the game was presented in the problem statement, and we have a few options for the exact method of implementation for the game including 2 different versions developed using pygame by Henry Charlesworth and by Karan Vombatkere, or a version using the openAI gymnasium by Gregory Eales. The links for all of these can be seen just below:\n",
    "\n",
    "https://github.com/Gregory-Eales/gym-catan\n",
    "\n",
    "https://github.com/henrycharlesworth/settlers_of_catan_RL\n",
    "\n",
    "https://github.com/kvombatkere/Catan-AI\n",
    "\n",
    "All of these appear to be completely functional and have been used for various things in the past, but depending on the performance during preliminary testing, we may decide that some of them are missing functionality or have slower performance. All of these have the ability to have human players, which is what we will use for testing against various types of opponents after training has been completed. Both pygame implementations have their own algorithm trained on their game that could additionally be used as a comparison for how our models learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Based on the examples we have seen, people tend to lean towards Monte Carlo methods as a solution given the nature of this problem. Since our goal for this project is to compare the models learning in a small group setting similar to a club or family learning the game together, we want each of the models to have a different kind of perspective on the game, as such the agents will be constructed as a split of Monte Carlo algorithms and Temporal Difference learning. Each of the agents within a single algorithm will have slightly different hyperparameters to create unique “personalities” for each agent. \n",
    "\n",
    "As a benchmark, we will have it play against 2 other algorithms and a player. Two of the three environments have their own sets of reinforcement learning algorithms that were used, and additionally, there is an available online version of the game that has bots for people to play against. These will be used as benchmark models, though more for comparison purposes than for competition, as the purpose of this project is to analyze the agents in the context of developing strategies within the group as opposed to being optimal agents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will use multiple evaluation metrics for our project. The first is the most straightforward: Win Rate. Quantized as games won divided by total games, win rate provides a metric to broadly evaluate the strategy of each of the agents we end up training at Catan. The goal for each agent is to win as many games as possible and this metric enables measuring alignment with that goal. The second evaluation metric is average victory points obtained per game. This metric evaluates how well each agent did per game regardless of whether it won. It will provide differentiation for strategies that win a similar percentage of the time and clarify how consistently a strategy places an agent in the top spots.  Finally, we will have the group of agents we trained play against existing online bots as well as humans to compare performance between each of the groups. The goal is for our agents to play competitively (i.e. have high win rates and average victory points obtained) against all groups. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project has a very low chance of violating ethics or privacy standards. It incorporates no user data that would be at risk of being leaked. The multiple agents being trained are constrained within their task environment and cannot have any impact beyond it. All of the software we plan to use for the project is open source and provided with the intention of free use. \n",
    "One potential area of impact for the project is on the online Settlers of Catan community. There are many people who play the game recreationally and even some who play professionally. We do not aim to disrupt these communities with our work. As seen in other games, input from AI can have positive and negative effects. In chess, computers have enabled deeper learning of the game while also enabling cheaters to win unfairly. We have no intention of releasing our Catan project publicly but are prepared to work towards developing ways to catch cheaters using AI if our project ends up helping cheaters make decisions in their games. This approach is similar to how cheating is regulated in online chess. There already exist bots that can play Settlers of Catan so the community of online players playing the game have had some time to adapt to the idea of playing against computers as well as humans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Team members complete high quality work on time\n",
    "* Team members communicate their needs via text\n",
    "* Work is divided as evenly as possible between team members\n",
    "* Criticism is constructive rather than destructive\n",
    "* Disagreements are resolved respectfully \n",
    "* If a team member cannot make it to a meeting they should inform the others ahead of time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 05/01  |  2 PM |  Research Topics  | Decide Project Topic | \n",
    "| 05/05  |  N/A |  Complete Individual Proposal Portions | Look Over Finished Proposal| \n",
    "| 05/10  | 2pm | N/A  | Incorporate Proposal Feedback   |\n",
    "| 05/17  | 2 PM  | Individual Work on Environments and Algorithm Implimentation | Check-in on Progress |\n",
    "| 05/24  | 2 PM  | Finalize Programming Work | Go Over Final Programming Needs|\n",
    "| 05/31  | 2 PM  | First Draft of Final Report| Go Over Edits to Final Report |\n",
    "| 06/07 | 2 PM  | Submit Report and Code |Final Meeing  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"firstnote\"></a>[1].[^](#firstnote): Kim, C., Li, A. (3 Dec 2021) Re-L Catan: Evaluation of Deep Reinforcement Learning for Resource Management Under Competitive and Uncertain Environments. <i>Stanford University</i>. <a href=\"http://cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf\">http://cs230.stanford.edu/projects_fall_2021/reports/103176936.pdf</a><br> \n",
    "\n",
    "<a name=\"secondnote\"></a>[2].[^](#secondnote): Pfeiffer, M. (1 Jan 1970) Reinforcement Learning of Strategies for Settlers of Catan. <i>Elsevier</i>. <a href=\"https://graz.elsevierpure.com/en/publications/reinforcement-learning-of-strategies-for-settlers-of-catan-2\">https://graz.elsevierpure.com/en/publications/reinforcement-learning-of-strategies-for-settlers-of-catan-2</a><br> \n",
    "\n",
    "<a name=\"thirdnote\"></a>[3].[^](#thirdnote): Chaslot, G. (2009) Monte-Carlo Tree Search in Settlers of Catan. <i>Tilburg University</i>. <a href=\"https://www.researchgate.net/publication/220716999_Monte-Carlo_Tree_Search_in_Settlers_of_Catan\">https://www.researchgate.net/publication/220716999_Monte-Carlo_Tree_Search_in_Settlers_of_Catan</a><br>  \n",
    "\n",
    "<a name=\"fourthnote\"></a>[4].[^](#fourthnote): Du Sautoy, M. (2023) 244,432,188,000 Games of Catan. <a href=\"https://www.youtube.com/watch?v=NkNoVnXfVXI\">https://www.youtube.com/watch?v=NkNoVnXfVXI</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
